==========
 硬體推薦
==========

Ceph 以一般硬體規格設計，這使部署、維護 PB 級別資料叢集的費用相對低廉。規劃叢集硬\
體時，需要平衡幾個方面因素，包括故障區域與潛在的校能問題。硬體規劃要包含把使\
用 Ceph 叢集的 Ceph 背景行程與其他行程妥善分散。通常我們推薦在一台機器上只執\
行一種類型的背景行程。我們推薦把使用資料叢集的行程（如 OpenStack、 \
CloudStack等雲端平台）安裝在別台機器上。


.. tip:: 關注 Ceph 的部落格，這些文章也值得參考，比如 `Ceph Write Throughput 1`_,
   `Ceph Write Throughput 2`_, `Argonaut v. Bobtail Performance Preview`_, 
   `Bobtail Performance - I/O Scheduler Comparison`_ and others are an
   excellent source of information. 


CPU
===

Ceph metadata 伺服器對 CPU 很敏感，它會動態地重新分散它們的負載，所以你的 metadata 伺服\
器應該要有足夠的處理能力（如四核心或更強悍的 CPU）。 Ceph 的 OSD 運行著 \
:term:`RADOS` 服務、用 :term:`CRUSH` 計算資料存放位置、複製資料與維護它自己的\
叢集狀態圖（Cluster Map）副本，因此 OSD 需要一定的處理能力（如雙核心 CPU）。監視器只簡單地維\
護著叢集狀態圖（Cluster Map）副本，因此對 CPU 需求不高；但必須考慮機器以後是否還會執行 Ceph \
監視器以外的 CPU 密集型任務。例如，如果伺服器以後還要運行用於計算的虛擬機（如 \
OpenStack Nova），你就要確保給 Ceph 行程保留了足夠的處理能力，所以我們推薦在\
其他機器上運行 CPU 密集型任務。


RAM 記憶體
=======

Metadata 伺服器和監視器（Monitor）必須能盡快地提供它們的資料，所以他們應該有足夠的記憶體，\
至少每個行程要 1GB，OSD 平時運作不需要這麼多記憶體（如每個行程 500MB 就差不多了）。\
然而在恢復期間它們佔用的記憶體比較大（如每個行程於每 TB 的資料需要約 1GB 記憶體）。一般記\
憶體越多越好。


資料儲存
========

要謹慎地規劃資料儲存配置，因為其之間涉及明顯的成本和效能折衷。來自作業系統的平\
行操作和到單個硬碟的多個背景行程平行讀取、寫入請求操作會極大地降低效能。檔案系統\
局限性也要考慮：btrfs 尚未穩定到可以用於生產環境的程度，但它可以寫入 journal 並\
同時寫入資料，而 xfs 和 ext4 卻不能。

.. important:: 因為 Ceph 發送 ACK 前必須把所有資料寫入 journal（至少對 xfs 和 \
   ext4 來說是），因此平衡 journal 和 OSD 效能相當重要。


硬碟驅動
----------

OSD 應該有足夠的空間用於儲存物件資料。考慮到大硬碟的每 GB 成本，我們建議用容\
量大於 1TB 的硬碟。建議用 GB 除以硬碟價格來計算每 GB 成本，因為較大的硬碟通\
常會對每 GB 成本有較大影響，例如，單價為 $75 的 1TB 硬碟其每 GB 價格為 $0.07 \
（$75/1024=0.0732），又如單價為 $150 的 3TB 硬碟其每 GB 價格為 $0.05 \
（$150/3072=0.0488），這樣使用 1TB 硬碟會增加 40% 的每 GB 價格，它將表現為\
較低的經濟性。另外單個驅動器容量越大，其對應的 OSD 所需記憶體就越大，特別是在\
重新平衡、資料回填與恢復期間。根據經驗，1TB 的存儲空間大約需要 1GB 記憶體。

.. tip:: 不顧分區，而在單個硬碟上運行多個 OSD，這樣是\ **不明智**\ ！

.. tip:: 不顧分區，而在運行了 SSD 的硬碟上同時運行監視器或 Metadata 伺服器也是\ \ **不明智**\ !

儲存驅動器受限於尋道時間（Seek Time）、存取時間、讀取寫入時間以及總吞吐量，這些物理局限性影\
響著整體系統效能，尤其在系統恢復期間。因此我們推薦獨立的驅動器用於安裝作業系\
統和軟體，另外每個 OSD 背景行程佔用一個驅動器。大多數 “slow OSD” 問題的起因都\
是在相同的硬碟上運行了作業系統、多個 OSD 或多個 journal 檔案。鑑於解決效能問\
題的成本差不多會超過另外增加磁碟驅動器，你應該在設計時就避免增加 OSD 儲存驅動\
器的負擔來提升效能。

Ceph 允許你在每塊硬碟驅動器上運行多個 OSD，但這會導致資源競爭並降低整體吞吐\
量；Ceph 也允許把 journal 和物件資料儲存在相同驅動器上，但這會增加記錄寫入 journal 並回\
應客戶端的延遲，因為 Ceph 必須先寫入 journal 才會回應確認了寫入動作。 btrfs 檔案系統\
能同時寫入 journal 資料和物件資料， xfs 和 ext4 卻不能。

Ceph 最佳實現指示，你應該分別在單獨的硬體運行作業系統、OSD 資料和 OSD journal。


固態硬碟
--------

一種提升效能的方法是使用固態硬碟（SSD）來降低隨機存取時間和讀取延遲，同時增加\
吞吐量。SSD 和硬碟相比每 GB 成本通常要高 10 倍以上，但存取時間至少比硬碟快 \
100 倍。

SSD 沒有可移動機械元件，所以不存在和硬碟一樣的局限性。但 SSD 也有局限性，評估\
SSD 時，循序讀寫效能很重要，在為多個 OSD 儲存 journal。 時，有著 400MB/s 順序讀寫吞\
吐量的 SSD 其效能遠高於 120MB/s 的。

.. important:: 我們建議發掘 SSD 的用法來提升效能。然而在大量投入 SSD 前，我\
   們\ **強烈建議**\ 核實 SSD 的效能指標，並在測試環境下平衡效能。

正因為 SSD 沒有移動機械元件，所以它很適合 Ceph 裡不需要太多儲存空間的地方。相\
對廉價的 SSD 很誘人，慎用！可接受的 IOPS 指標對選擇用於 Ceph 的 SSD 還不夠，\
用於 journal。 和 SSD 時還有幾個重要考量：

- **寫入密集語義（Write-intensive semantics）：** 寫入 journal。涉及寫入密集語義，所以你要確保選用的 SSD 寫入效能和硬碟相當或好於硬碟。廉價 SSD 可能在加速存取的同時引入寫入延遲，有時候高效能硬碟的寫入速度可以和便宜 SSD 互相媲美。

- **循序寫入：** 在一個 SSD 上為多個 OSD 儲存多個 journal 時也必須考慮 SSD 的循序\
  寫入極限，因為它們要同時處理多個 OSD journal 的寫入請求。

- **分區對齊（Partition Alignment）：** 採用了SSD 的一個常見問題是人們喜歡分區，卻常常忽略了分區對齊，這會導致 SSD 的資料傳輸速率慢很多，所以請確保分區​​對齊了。

SSD 用於物件儲存太昂貴了，但是把 OSD 的 journal 存到 SSD、把物件資料儲存到獨立的\
硬碟可以明顯提升效能。 ``osd journal`` 選項的預設值是 \
``/var/lib/ceph/osd/$cluster-$id/journal``，你可以把它掛載到一個 SATA 或 SSD \
分區，這樣它就不再是和物件儲存一樣儲存在同一個硬碟上的檔案了

提升 CephFS 檔案系統效能的一種方法是從 CephFS 檔案內容裡分離出 metadata。 Ceph \
提供了預設的 ``metadata`` 儲存池來儲存 CephFS metadata，所以你不需要給 CephFS \
metadata 建立儲存池，但是可以給它建立一個僅指向某台主機 SSD 的 CRUSH Map。詳細\
請看\ `給儲存池指定 OSD`_ 。


控制器
------

硬碟控制器對寫入吞吐量也有顯著影響，要謹慎地選擇，以免產生效能瓶頸。

.. tip:: Ceph blog 一般是優秀的 Ceph 效能問題來源，請看 `Ceph Write Throughput 1`_ \
   與 `Ceph Write Throughput 2`_ 。


其他注意事項
------------

你可以在同一台主機上運行多個 OSD，但要確保 OSD 硬碟總吞吐量不超過為客戶端提供\
讀取寫入服務所需的網路頻寬；還要考慮叢集在每台主機上所儲存的資料佔總體的百分比，\
如果一台主機所佔百分比太大而它掛了，就可能導致諸如超過 ``full ratio`` 的問題，\
此問題會使 Ceph 中止運作以防資料遺失。

如果每台主機運行多個 OSD ，也得保證核心是最新的。參考\ `作業系統推薦`_\ 裡關\
於 ``glibc`` 和 ``syncfs(2)`` 的部分，確保硬體效能可達到期望值。

OSD 數量較多（如 20 個以上）的主機會派生出大量執行緒，尤其​​是在恢復和重新平衡期\
間。很多 Linux 核心預設的最大執行緒數較小（如 32k 個），如果您遇到了這類問題，\
可以把 ``kernel.pid_max`` 值調高些。理論最大值是 4194303 。例如把下列這行加\
入 ``/etc/sysctl.conf`` 檔案： ::

	kernel.pid_max = 4194303


網路
====

建議每台機器最少兩個 1G 網卡，現在大多數機械硬碟都能達到大概 100MB/s 的吞吐\
量，網卡應要能處理所有 SSD 硬碟總吞吐量，所以推薦最少兩個 1G 網卡，分別用於\
公共網路（前端）和叢集網路（後端）。叢集網路（最好別連接到 Internet）用於處理由\
資料複製產生的額外負載，而且可防止阻斷式服務攻擊，阻斷式服務攻擊會干擾資料放置\
群組，使之在 OSD 資料複製時不能回到 ``active + clean`` 狀態。請考慮部署 10G 網\
卡。通過 1Gbps 網路複製 1TB 資料需耗時 3 小時，而 3TB （典型配置）需要 9 小時，\
相比之下，如果使用 10Gbps 複製時間可分別縮短到 20 分鐘和 1 小時。在一個 PB \
級叢集中，OSD 磁碟故障是常見的，而非異常；在效價比合理的的前提下，系統管理者\
想讓 PG 盡快從 ``degraded`` （降級）狀態恢復到 ``active + clean`` 狀態。另\
外，一些部署工具（如 Dell 的 Crowbar）部署了五個不同的網路，但使用了 VLAN \
以提高網路和硬體可管理性。VLAN 使用 802.1q 協定，還需要採用支援 VLAN 功能的\
網卡和交換器，增加的硬體成本可用節省的運營（網路安裝、維護）成本抵消。使用 \
VLAN 來處理叢集和運算堆疊（如 OpenStack、CloudStack 等等）之間的 VM 流量時，\
採用 10G 網卡仍然值得。每個網路的機架路由器到核心路由器應該有更大的頻寬，如 \
40Gbps 到 100Gbps 的。

伺服器應要配置主板管理控制器（Baseboard Management Controller, BMC），管理和\
部署工具也應該在大規模使用 BMC，所以請考慮到外部網路管理的成本以及效益平衡，此程序\
管理著 SSH 存取、 VM 映像檔上傳、作業系統安裝與埠口管理等等，會徒增網路負載。 \
運營三個網路有點過分，但是每條流量路徑都指示了部署一個大型資料叢集前要仔細考\
慮的潛在能力、吞吐量與效能瓶頸。


故障區域（FAILURE DOMAINS）
======

故障區域指任何導致不能存取一個或多個 OSD 的故障，可以是主機停止的行程、硬體故\
障、作業系統當機、有問題的網卡、損壞的電源、網路中斷​與斷電等等。規劃硬體需求時，\
要在多個需求之間尋求平衡點，像付出很多努力減少故障區域帶來的成本削減、隔離每個潛\
在故障區域增加的成本。


最低硬體推薦
============

Ceph 可以運行在廉價的一般硬體上，小型生產叢集和開發叢集可以在一般的硬體上。

+--------------+----------------+-----------------------------------------+
|  行程        | 條件           | 最低建議                                |
+==============+================+=========================================+
| ``ceph-osd`` | Processor      | - 1x 64-bit AMD-64                      |
|              |                | - 1x 32-bit ARM dual-core or better     |
|              |                | - 1x i386 dual-core                     |
|              +----------------+-----------------------------------------+
|              | RAM            |  ~1GB for 1TB of storage per daemon     |
|              +----------------+-----------------------------------------+
|              | Volume Storage |  1x storage drive per daemon            |
|              +----------------+-----------------------------------------+
|              | Journal        |  1x SSD partition per daemon (optional) |
|              +----------------+-----------------------------------------+
|              | Network        |  2x 1GB Ethernet NICs                   |
+--------------+----------------+-----------------------------------------+
| ``ceph-mon`` | Processor      | - 1x 64-bit AMD-64/i386                 |
|              |                | - 1x 32-bit ARM dual-core or better     |
|              |                | - 1x i386 dual-core                     |
|              +----------------+-----------------------------------------+
|              | RAM            |  1 GB per daemon                        |
|              +----------------+-----------------------------------------+
|              | Disk Space     |  10 GB per daemon                       |
|              +----------------+-----------------------------------------+
|              | Network        |  2x 1GB Ethernet NICs                   |
+--------------+----------------+-----------------------------------------+
| ``ceph-mds`` | Processor      | - 1x 64-bit AMD-64 quad-core            |
|              |                | - 1x 32-bit ARM quad-core               |
|              |                | - 1x i386 quad-core                     |
|              +----------------+-----------------------------------------+
|              | RAM            |  1 GB minimum per daemon                |
|              +----------------+-----------------------------------------+
|              | Disk Space     |  1 MB per daemon                        |
|              +----------------+-----------------------------------------+
|              | Network        |  2x 1GB Ethernet NICs                   |
+--------------+----------------+-----------------------------------------+

.. tip:: 如果在只有一塊硬碟的機器上運行 OSD，要把資料與作業系統分別放到不同\
   分區；一般來說，我們推薦作業系統和資料分別使用不同的硬碟。


生產環境叢集實例
============

PB 級別生產叢集也可以使用一般硬體，但應該配備更多記憶體、CPU 和資料儲存空間來解\
決流量壓力。


Dell 實例
---------

一個最新（2012）的 Ceph 叢集專案使用了 2 個相當強悍的 OSD 硬體配備與較輕量\
的監視器（Monitor）配備。

+----------------+----------------+------------------------------------+
|  Configuration | Criteria       | Minimum Recommended                |
+================+================+====================================+
| Dell PE R510   | Processor      |  2x 64-bit quad-core Xeon CPUs     |
|                +----------------+------------------------------------+
|                | RAM            |  16 GB                             |
|                +----------------+------------------------------------+
|                | Volume Storage |  8x 2TB drives. 1 OS, 7 Storage    |
|                +----------------+------------------------------------+
|                | Client Network |  2x 1GB Ethernet NICs              |
|                +----------------+------------------------------------+
|                | OSD Network    |  2x 1GB Ethernet NICs              |
|                +----------------+------------------------------------+
|                | Mgmt. Network  |  2x 1GB Ethernet NICs              |
+----------------+----------------+------------------------------------+
| Dell PE R515   | Processor      |  1x hex-core Opteron CPU           |
|                +----------------+------------------------------------+
|                | RAM            |  16 GB                             |
|                +----------------+------------------------------------+
|                | Volume Storage |  12x 3TB drives. Storage           |
|                +----------------+------------------------------------+
|                | OS Storage     |  1x 500GB drive. Operating System. |
|                +----------------+------------------------------------+
|                | Client Network |  2x 1GB Ethernet NICs              |
|                +----------------+------------------------------------+
|                | OSD Network    |  2x 1GB Ethernet NICs              |
|                +----------------+------------------------------------+
|                | Mgmt. Network  |  2x 1GB Ethernet NICs              |
+----------------+----------------+------------------------------------+





.. _Ceph Write Throughput 1: http://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/
.. _Ceph Write Throughput 2: http://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/
.. _Argonaut v. Bobtail Performance Preview: http://ceph.com/uncategorized/argonaut-vs-bobtail-performance-preview/
.. _Bobtail Performance - I/O Scheduler Comparison: http://ceph.com/community/ceph-bobtail-performance-io-scheduler-comparison/
.. _給儲存池指定 OSD: http://ceph.com/docs/master/rados/operations/crush-map/#placing-different-pools-on-different-osds
.. _作業系統推薦: ../os-recommendations
