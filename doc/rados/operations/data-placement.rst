==============
 数据归置概览
==============

Ceph 通过 RADOS 集群动态地存储、复制和重新均衡数据对象。很多不同用户因不同目的把对\
象存储在不同的存储池里，而它们都坐落于无数的 OSD 之上，所以 Ceph 的运营需要些数据归\
置计划。 Ceph 的数据归置计划概念主要有：

- **存储池（ Pool ）：** Ceph 在存储池内存储数据，它是对象存储的逻辑组；存储池管理\
  着归置组数量、副本数量、和存储池规则集。要往存储池里存数据，用户必须通过认证、且权\
  限合适，存储池可做快照。详情参见\ `存储池`_\ 。

- **归置组（ Placement Group ）：** Ceph 把对象映射到归置组（ PG ），归置组是一逻\
  辑对象池的片段，这些对象组团后再存入到 OSD 。归置组减少了各对象存入对应 OSD 时的\
  元数据数量，更多的归置组（如每 OSD 100 个）使得均衡更好。详情见\ `归置组`_\ 。

- **CRUSH 图（ CRUSH Map ）：** CRUSH 是重要组件，它使 Ceph 能伸缩自如而没有性能\
  瓶颈、没有扩展限制、没有单点故障，它为 CRUSH 算法提供集群的物理拓扑，以此确定一个\
  对象的数据及它的副本应该在哪里、怎样跨故障域存储，以提升数据安全。详情见 \
  `CRUSH 图`_\ 。

起初安装测试集群的时候，可以使用默认值。但开始规划一个大型 Ceph 集群，做数据归置操\
作的时候会涉及存储池、归置组、和 CRUSH 。如果您遇到难题， `Inktank`_ 可提供有偿技\
术支持。


.. _存储池: ../pools
.. _归置组: ../placement-groups
.. _CRUSH 图: ../crush-map
.. _Inktank: http://www.inktank.com
